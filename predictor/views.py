# predictor/views.py

from django.shortcuts import render
from django.http import JsonResponse, HttpResponse
import joblib
import pandas as pd
import numpy as np
import os
from django.conf import settings
import json # Import the json library to parse JSON data
import traceback # For more detailed error logging if needed

# --- Define paths to your saved model, columns, and scaler ---
# These paths are relative to your project's BASE_DIR
MODEL_PATH = os.path.join(settings.BASE_DIR, 'saved_models', 'california_housing_model_rf.joblib')
COLUMNS_PATH = os.path.join(settings.BASE_DIR, 'saved_models', 'california_model_feature_columns.joblib')
SCALER_PATH = os.path.join(settings.BASE_DIR, 'saved_models', 'california_housing_scaler.joblib')

# --- Load the model, columns, and scaler when the Django app starts ---
# This is more efficient than loading them on every request
try:
    model = joblib.load(MODEL_PATH)
    model_columns = joblib.load(COLUMNS_PATH) # This is the list of column names in the correct order
    scaler = joblib.load(SCALER_PATH)
    print("Model, columns, and scaler loaded successfully at Django startup.")
except Exception as e:
    print(f"FATAL ERROR loading model/columns/scaler at Django startup: {e}")
    print(traceback.format_exc()) # Print full traceback for critical errors
    model = None
    model_columns = None
    scaler = None

def home(request):
    # This view will now render our HTML page
    return render(request, 'predictor/index.html')

def predict_price(request):
    if model is None or model_columns is None or scaler is None:
        return JsonResponse({'error': 'Model or supporting files not loaded correctly. Please check server logs.'}, status=500)

    if request.method == 'POST':
        try:
            # Load JSON data from the request body
            data_from_frontend = json.loads(request.body)

            # Basic validation for required raw input keys
            required_raw_keys = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 
                                 'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity']
            for key in required_raw_keys:
                if key not in data_from_frontend:
                    return JsonResponse({'error': f'Missing key in input data: {key}'}, status=400)

            # Create a DataFrame from the input data (single row)
            # JavaScript already did parseFloat, so numerical strings should be numbers here if JS worked correctly.
            input_df_raw = pd.DataFrame([data_from_frontend])

            # --- Preprocessing the input data ---
            input_df_processed = input_df_raw.copy()

            # 1. Feature Engineering (same as in the notebook)
            # Ensure inputs for division are not zero or handle appropriately
            if input_df_processed['households'].iloc[0] == 0 or input_df_processed['total_rooms'].iloc[0] == 0:
                return JsonResponse({'error': 'Households or Total Rooms cannot be zero for feature engineering.'}, status=400)
                
            input_df_processed['rooms_per_household'] = input_df_processed['total_rooms'] / input_df_processed['households']
            input_df_processed['bedrooms_per_room'] = input_df_processed['total_bedrooms'] / input_df_processed['total_rooms']
            input_df_processed['population_per_household'] = input_df_processed['population'] / input_df_processed['households']

            # 2. One-Hot Encoding for 'ocean_proximity'
            input_ocean_proximity_value = input_df_processed.pop('ocean_proximity').iloc[0]
            
            # Initialize all possible OHE columns (derived from model_columns) to False
            for col_name in model_columns:
                if col_name.startswith('ocean_proximity_'):
                    input_df_processed[col_name] = False 
            
            # Set the specific OHE column to True based on input.
            # This matches the column names generated by pd.get_dummies(..., drop_first=True)
            # e.g., 'ocean_proximity_INLAND', 'ocean_proximity_NEAR_BAY', etc.
            # If the input category was the one dropped, all these flags remain False.
            constructed_ohe_col_name = f"ocean_proximity_{input_ocean_proximity_value.upper().replace(' ', '_').replace('<', 'LT')}"
            if constructed_ohe_col_name in input_df_processed.columns: # Check if it's one of the non-dropped columns
                 input_df_processed[constructed_ohe_col_name] = True

            # 3. Create final DataFrame with correct columns and dtypes
            # Initialize a dictionary for the row data
            input_data_for_model_row = {}
            for col_name in model_columns:
                if col_name in input_df_processed.columns:
                    input_data_for_model_row[col_name] = input_df_processed.loc[0, col_name]
                else:
                    # This path should ideally not be hit often if input_df_processed is built correctly
                    # It's mainly for OHE columns that should be False by default.
                    input_data_for_model_row[col_name] = False 

            # Create the DataFrame with columns in the order the model expects
            final_input_df = pd.DataFrame([input_data_for_model_row], columns=model_columns)
            
            # Identify columns that were scaled during training
            # Using scaler.feature_names_in_ is the most robust if available
            if hasattr(scaler, 'feature_names_in_'):
                cols_to_scale = [col for col in scaler.feature_names_in_ if col in final_input_df.columns]
            else: # Fallback if scaler.feature_names_in_ is not available
                cols_to_scale = [
                    'longitude', 'latitude', 'housing_median_age', 'total_rooms', 
                    'total_bedrooms', 'population', 'households', 'median_income', 
                    'rooms_per_household', 'bedrooms_per_room', 'population_per_household'
                ]
                # Filter to only those present in our current DataFrame
                cols_to_scale = [col for col in cols_to_scale if col in final_input_df.columns]

            # Explicitly set dtypes before scaling and for OHE
            for col in final_input_df.columns:
                if col in cols_to_scale:
                    final_input_df[col] = final_input_df[col].astype(float)
                elif col.startswith('ocean_proximity_'):
                    final_input_df[col] = final_input_df[col].astype(bool)
                # Any other columns (if they existed and weren't scaled or OHE) would retain inferred type.
                # For this dataset, all features in model_columns are either scaled or OHE.

            # 4. Feature Scaling
            if cols_to_scale: # If there are columns to scale
                final_input_df[cols_to_scale] = scaler.transform(final_input_df[cols_to_scale])
            else:
                print("Warning: No columns were identified or available for scaling in the input.")
            
            # Make Prediction
            # Ensure the columns passed to predict are exactly what the model expects, in order
            prediction = model.predict(final_input_df[model_columns])

            return JsonResponse({'prediction': prediction[0]})

        except json.JSONDecodeError:
            return JsonResponse({'error': 'Invalid JSON received from frontend.'}, status=400)
        except KeyError as e:
            return JsonResponse({'error': f'Missing expected data field: {str(e)}'}, status=400)
        except ValueError as e: # Catch potential errors from astype or division by zero if not caught earlier
             print(f"ValueError during processing: {e}")
             traceback.print_exc()
             return JsonResponse({'error': f'Invalid data value or type: {str(e)}'}, status=400)
        except Exception as e:
            # Log the full error for debugging on the server
            print(f"Unexpected error during prediction: {e}")
            traceback.print_exc()
            return JsonResponse({'error': 'An unexpected error occurred on the server.'}, status=500)
    else:
        return JsonResponse({'error': 'Only POST requests are accepted for prediction.'}, status=405)